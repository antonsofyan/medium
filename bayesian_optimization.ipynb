{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Requirements: \n",
    "\n",
    "scipy=1.4.1\n",
    "imageio=2.9.0\n",
    "numpy=1.19.1 \n",
    "pandas=1.0.5\n",
    "matplotlib=3.3.1\n",
    "sklearn=0.23.2\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as scs\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, ConstantKernel as C\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import imageio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    The script for BayesOptimization\n",
    "    Main functions:\n",
    "    bayes_opt:\n",
    "        inputs: path_to_predictions,search_space,list_of_parameters_names\n",
    "        operation: given the point evaluation of the function in a csv file\n",
    "        and search parameters, return the next point for function evaluation.\n",
    "\"\"\"\n",
    "def bayes_opt(path_to_predictions,search_space,list_of_parameters_names,maximize):\n",
    "    # If the prediction file exists, run bayes optimisation else create a new one\n",
    "    #print(\"PATH EXISTS: \",os.path.exists(path_to_predictions))\n",
    "    #print(path_to_predictions)\n",
    "    dict_of_means = {}\n",
    "    for key in list_of_parameters_names:\n",
    "        \n",
    "        dict_of_means[key] = [float(search_space[key][0][1]+search_space[key][0][0])/2.0,\n",
    "                              float(search_space[key][0][1]-search_space[key][0][0])/2.0]\n",
    "    \n",
    "    if os.path.exists(path_to_predictions):\n",
    "        \n",
    "        # Read the csv with predictions\n",
    "        parameters_and_loss= pd.read_csv(path_to_predictions,delimiter=',')\n",
    "        \n",
    "        # Convert to dictionary\n",
    "        parameters_and_loss_dict = parameters_and_loss.to_dict('list')\n",
    "    \n",
    "        # Generate function prediction from the parameters and the loss (run bayesian regression)\n",
    "        loss_predicted,sigma,loss_evaluated,list_of_ranges = generate_prediction(search_space,parameters_and_loss_dict,dict_of_means,list_of_parameters_names)\n",
    "        \n",
    "        # Calculate expected improvement (finding the maximum of the information gain function)\n",
    "        expected_improvement = calculate_expected_improvement(loss_predicted, sigma, loss_evaluated)\n",
    "        \n",
    "        # Find the parameter values for the maximum values of the information gain function\n",
    "        next_parameter_values = find_next_parameter_values(expected_improvement,list_of_ranges,list_of_parameters_names,parameters_and_loss,maximize)\n",
    "        \n",
    "    else:\n",
    "\n",
    "        parameters_and_loss = pd.DataFrame(columns = list_of_parameters_names+['loss'])\n",
    "        next_parameter_values = {}\n",
    "        for name in list_of_parameters_names:\n",
    "            next_parameter_values[name] = search_space[name][1](search_space[name][0][0]+(search_space[name][0][1]-search_space[name][0][0])*np.random.uniform(0,1))\n",
    "        \n",
    "        loss_predicted = None\n",
    "        sigma=None\n",
    "        loss_evaluated=None\n",
    "        expected_improvement=None   \n",
    "        \n",
    "    # Write the results into the dataframe\n",
    "    list_of_next_values = []\n",
    "\n",
    "    for item in list_of_parameters_names:\n",
    "        # Add all parameters to the list, so that they can be appended to the dataframe\n",
    "        # While iterating convert to the preffered datatype\n",
    "        list_of_next_values.append(search_space[item][1](round(next_parameter_values[item],5)))\n",
    "        next_parameter_values[item] = (search_space[item][1](round(next_parameter_values[item],5)))\n",
    "\n",
    "    # Check that the output is not repeated (can happen as we are using white noize kernel) in this case generate point at random\n",
    "    while (parameters_and_loss[list_of_parameters_names]== list_of_next_values).all(1).any():\n",
    "        list_of_next_values = []\n",
    "        for item in list_of_parameters_names:\n",
    "            rand_value = search_space[item][1](round(random.uniform(search_space[item][0][0], search_space[item][0][1]),5))\n",
    "            next_parameter_values[item] = rand_value\n",
    "            list_of_next_values.append(rand_value)\n",
    "\n",
    "    parameters_and_loss= parameters_and_loss.append(pd.DataFrame(data= [list_of_next_values],\n",
    "                                                                 columns = list_of_parameters_names),\n",
    "                                                    sort=False,\n",
    "                                                    ignore_index=True)\n",
    "\n",
    "    parameters_and_loss.to_csv(path_to_predictions,index=False)\n",
    "    \n",
    "    return next_parameter_values,loss_predicted,sigma,loss_evaluated,expected_improvement,dict_of_means\n",
    "\n",
    "\n",
    "def generate_meshes(parameters_and_loss, search_space,dict_of_means, list_of_parameters_names):\n",
    "    # Create a list of estimation parameters, iterate over all, skip the loss from the list\n",
    "    list_of_parameters = []\n",
    "    \n",
    "    # Create the list of ranges for the search space (start, end, number_of_points)\n",
    "    list_of_ranges = []\n",
    "    list_of_shapes = []\n",
    "    list_of_ranges_true =[]\n",
    "    \n",
    "    search_space_normalized = {}\n",
    "    for key in list_of_parameters_names:\n",
    "        \n",
    "        search_space_normalized[key] = [(search_space[key][0][0]-dict_of_means[key][0])/dict_of_means[key][1],\n",
    "                                   (search_space[key][0][1]-dict_of_means[key][0])/dict_of_means[key][1],\n",
    "                                   (search_space[key][0][2])]\n",
    "        #print(search_space_normalized)\n",
    "        list_of_ranges_true.append (np.linspace(*search_space[key][0]))\n",
    "        list_of_ranges.append(np.linspace(*search_space_normalized[key]))\n",
    "        list_of_shapes.append(search_space[key][0][2])\n",
    "        \n",
    "    normalized_param= []\n",
    "    \n",
    "    for key in list_of_parameters_names:\n",
    "        normalized_param=[(float(param)-dict_of_means[key][0])/dict_of_means[key][1] for param in parameters_and_loss[key]]\n",
    "        \n",
    "        list_of_parameters.append(np.atleast_2d(normalized_param).T)\n",
    "    \n",
    "    list_of_parameters_stack = np.stack(list_of_parameters,axis=1)\n",
    "\n",
    "    # Create a meshgrid from the list of ranges for the searchspace\n",
    "    meshgrid_linspace = np.meshgrid(*(list_of_ranges),indexing='ij')\n",
    "\n",
    "    reshape_param = np.product(np.shape(meshgrid_linspace[0]))\n",
    "    meshgrid_linspacer = []\n",
    "    for mlinsp in meshgrid_linspace:\n",
    "        meshgrid_linspacer.append(np.reshape(mlinsp,reshape_param))\n",
    "\n",
    "    # meshgrid for GP prediction\n",
    "    meshgrid_linspacer_stack = np.stack(meshgrid_linspacer,axis=1)\n",
    "    \n",
    "    return list_of_parameters_stack, list_of_parameters, list_of_shapes, meshgrid_linspacer_stack, list_of_ranges_true\n",
    "    \n",
    "\n",
    "def fit_gp(list_of_parameters_stack, list_of_parameters, list_of_shapes, loss_evaluated, meshgrid_linspacer_stack):\n",
    "    \n",
    "    # Instantiate a Gaussian Process model\n",
    "    kernel = kernel =  RBF(5, (1e-2, 1e2))*C(1, (1e-2, 1e2))+ WhiteKernel(noise_level=0.2)\n",
    "    \n",
    "    gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=200)\n",
    "\n",
    "    # Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "    gp.fit(np.reshape(list_of_parameters_stack,(-1,len(list_of_parameters))), loss_evaluated)\n",
    "\n",
    "    # Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "    loss_predicted, sigma = gp.predict(meshgrid_linspacer_stack, return_std=True)\n",
    "    \n",
    "    loss_predicted = np.reshape(loss_predicted,list_of_shapes)\n",
    "    sigma = np.reshape(sigma,list_of_shapes)\n",
    "\n",
    "    return loss_predicted, sigma \n",
    "      \n",
    "    \n",
    "def generate_prediction(search_space, parameters_and_loss,dict_of_means,list_of_parameters_names):\n",
    "\n",
    "    list_of_parameters_stack, list_of_parameters, list_of_shapes, meshgrid_linspacer_stack, list_of_ranges_true = generate_meshes(parameters_and_loss,\n",
    "                                                                                                                                 search_space, dict_of_means,\n",
    "                                                                                                                                 list_of_parameters_names)\n",
    "    loss_evaluated = parameters_and_loss['loss']\n",
    "    if len(parameters_and_loss['loss'])>1:\n",
    "        loss_evaluated = (parameters_and_loss['loss']-np.mean(parameters_and_loss['loss']))/(np.std(parameters_and_loss['loss'])+1e-6)\n",
    "    \n",
    "    loss_predicted, sigma  = fit_gp(list_of_parameters_stack, list_of_parameters, list_of_shapes, loss_evaluated, meshgrid_linspacer_stack)\n",
    "    \n",
    "    return loss_predicted,sigma,loss_evaluated,list_of_ranges_true\n",
    "\n",
    "\n",
    "def calculate_expected_improvement(loss_predicted, sigma, loss_evaluated):\n",
    "    \n",
    "    # Calculate the expected improvement\n",
    "    eps = 1e-6\n",
    "    num =(loss_predicted-max(loss_evaluated)-eps)\n",
    "    Z=num/sigma\n",
    "    expected_improvement = num*scs.norm(0,1).cdf(Z)+sigma*scs.norm(0,1).pdf(Z)\n",
    "    expected_improvement[sigma==0.0] = 0.0\n",
    "\n",
    "    return expected_improvement\n",
    "\n",
    "\n",
    "def find_next_parameter_values(expected_improvement,parameter_values,list_of_parameters_names,parameters_and_loss,maximize):\n",
    "\n",
    "    if maximize:\n",
    "        index = np.where(expected_improvement==np.amax(expected_improvement))\n",
    "    else:\n",
    "        index = np.where(expected_improvement==np.amin(expected_improvement))\n",
    "\n",
    "    next_parameter_values = {}\n",
    "    # Iterate over all parameter values and find those corresponding to maximum EI\n",
    "    for idx, parameter in enumerate(list_of_parameters_names):\n",
    "        \n",
    "        # Since more than one value can be have max at EI,select one at random\n",
    "        x = int(np.random.uniform(0,len(index[idx])))\n",
    "        next_parameter_values[parameter] = parameter_values[idx][index[idx][x]]\n",
    "\n",
    "    return next_parameter_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_dim_function_simple(burnin):\n",
    "    '''\n",
    "    Maximum is attained at 12000\n",
    "    '''\n",
    "    f = -((burnin-12000)/10000)**2\n",
    "    return f\n",
    "\n",
    "\n",
    "def one_dim_function_complex(burnin):\n",
    "    '''\n",
    "    Maximum is attained at 12000\n",
    "    '''\n",
    "    f = norm.pdf((burnin-12000)/1000)+np.sin((burnin-12000)/500)/5\n",
    "    return f\n",
    "\n",
    "\n",
    "def two_dim_function_simple(burnin, learning_rate):\n",
    "    '''\n",
    "    Maximum is attained at 12000, 1.5\n",
    "    '''\n",
    "    f = (-((burnin-12000)/10000)**2)+(-(learning_rate-1.25)**2)\n",
    "    return f\n",
    "\n",
    "\n",
    "def two_dim_function_complex(burnin, learning_rate):\n",
    "    '''\n",
    "    Maximum is attained at 12000, 1.5\n",
    "    '''\n",
    "    \n",
    "    x = (burnin-12000)/1000\n",
    "    y = (learning_rate-1)*2-1.5\n",
    "    \n",
    "    f = (1-(x**2+y**3))*np.exp(-(x**2+y**2)/2)\n",
    "\n",
    "    return f\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1D Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "number_of_iterations = 15\n",
    "\n",
    "\n",
    "# Dictionary:\n",
    "# Key: parameter name\n",
    "# Entry: tupple with (i) a list for the search space interval (start_val, end_val, num_points) and (ii) type of parameter\n",
    "search_space = {'burnin_period':([8000, 16000, 100],int)}\n",
    "maximize=True\n",
    "\n",
    "list_of_parameters_names = ['burnin_period']\n",
    "path_to_predictions = 'test.csv'\n",
    "\n",
    "\n",
    "if not os.path.isdir('figures_1D'):\n",
    "    os.mkdir('./figures_1D/')\n",
    "\n",
    "if os.path.isfile(path_to_predictions):\n",
    "    os.remove(path_to_predictions)\n",
    "\n",
    "\n",
    "for ii in range(0, number_of_iterations):\n",
    "    print(ii)\n",
    "    \n",
    "    # Fit and get the next point to sample at\n",
    "    params,loss_predicted,sigma,loss_evaluated,expected_improvement,dict_of_means = bayes_opt(path_to_predictions,search_space,list_of_parameters_names,maximize)\n",
    "    \n",
    "    \n",
    "    # Generate the predition from the true function\n",
    "    res = one_dim_function_complex(params['burnin_period'])\n",
    "    \n",
    "    # Read the data already sampled\n",
    "    data = pd.read_csv(path_to_predictions, delimiter =',')\n",
    "    \n",
    "    mean_loss =np.mean(data.loc[0:(len(data)-2),'loss'].values)\n",
    "    std_loss =np.std(data.loc[0:(len(data)-2),'loss'].values)\n",
    "\n",
    "    # Append the new datapoint to the dataframe and save\n",
    "    data.at[len(data)-1, 'loss'] = res\n",
    "    data.to_csv(path_to_predictions, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot and save the fit\n",
    "    x = np.linspace(*search_space['burnin_period'][0])    \n",
    "    plt.figure()\n",
    "    plt.plot(x, one_dim_function_complex(x),'b',label=\"True function\");\n",
    "    plt.plot(data['burnin_period'].values,data['loss'].values,'*r',label=\"Collected measurements\")\n",
    "    plt.plot(data['burnin_period'].values[-1],data['loss'].values[-1],'og',label=\"New location\")\n",
    "        \n",
    "    if ii>0:\n",
    "        if ii>1:\n",
    "            loss_predicted= loss_predicted*std_loss+mean_loss\n",
    "            sigma= sigma*std_loss+mean_loss        \n",
    "\n",
    "        plt.plot(x,loss_predicted,'r',label=\"Predicted function\")\n",
    "        plt.fill_between(x, loss_predicted - sigma,\n",
    "                 loss_predicted + sigma,\n",
    "                 alpha=0.5, color='c',label=\"Standard deviation\")  \n",
    "    plt.ylim([-.5, .8])\n",
    "    plt.grid()\n",
    "    plt.ylabel('Loss', fontsize=16)\n",
    "    plt.xlabel('Burn-in period', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.legend(loc = 'upper left')\n",
    "    plt.savefig('figures_1D/'+str(ii)+'.png', bbox_inches='tight',dpi=300)\n",
    "    \n",
    "\n",
    "# Write out images to a gif\n",
    "images = []\n",
    "for ii in range(0,number_of_iterations):\n",
    "    filename = 'figures_1D/'+str(ii)+'.png'\n",
    "    image = imageio.imread(filename)\n",
    "    images.append(image)\n",
    "        \n",
    "imageio.mimsave(('movie.gif'), images, duration = 1) # modify duration as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, one_dim_function_complex(x),'b',label=\"True function\");\n",
    "plt.plot(data['burnin_period'].values[:-1],data['loss'].values[:-1],'*r',label=\"Data\")\n",
    "        \n",
    "plt.grid()\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Burn-in period', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.ylim([-.5, .8])\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.savefig('./fig_1.png', bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, expected_improvement,'b',label=\"\");\n",
    "#plt.plot(data['burnin_period'].values[:-1],data['loss'].values[:-1],'*r',label=\"Data\")\n",
    "        \n",
    "plt.grid()\n",
    "plt.ylabel('Expected Improvement', fontsize=16)\n",
    "plt.xlabel('Burn-in period', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.savefig('./fig_2.png', bbox_inches='tight',dpi=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(x, one_dim_function_complex(x),'b',label=\"True function\");\n",
    "plt.plot(data['burnin_period'].values[:-1],data['loss'].values[:-1],'*r',label=\"Data\")\n",
    "plt.plot(x,loss_predicted,'r',label=\"Predicted function\")\n",
    "plt.fill_between(x, loss_predicted - sigma,\n",
    "         loss_predicted + sigma,\n",
    "         alpha=0.5, color='c',label=\"Standard deviation\")  \n",
    "plt.grid()\n",
    "plt.ylim([-.5, .8])\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.xlabel('Burn-in period', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.legend(fontsize=16)\n",
    "\n",
    "plt.savefig('./fig_3.png', bbox_inches='tight',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2D Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import imageio\n",
    "\n",
    "if os.path.isfile(path_to_predictions):\n",
    "    os.remove(path_to_predictions)\n",
    "\n",
    "\n",
    "number_of_iterations = 30\n",
    "\n",
    "# Dictionary:\n",
    "# Key: parameter name\n",
    "# Entry: tupple with (i) a list for the search space interval (start_val, end_val, num_points) and (ii) type of parameter\n",
    "search_space = {'burnin_period':([8000, 16000, 100],int),\n",
    "                'learning_rate':([0.5, 2.0, 100],float)}\n",
    "\n",
    "if not os.path.isdir('figures_1D'):\n",
    "    os.mkdir('./figures_2D/')\n",
    "    \n",
    "maximize=True\n",
    "\n",
    "list_of_parameters_names = ['burnin_period','learning_rate']\n",
    "path_to_predictions = 'test.csv'\n",
    "    \n",
    "for ii in range(0, number_of_iterations):\n",
    "    print(ii)\n",
    "    \n",
    "    # Generate the next point to sample\n",
    "    params,loss_predicted,_,loss_evaluated,expected_improvement,_ = bayes_opt(path_to_predictions,search_space,list_of_parameters_names,maximize)\n",
    "    \n",
    "    # Measure the result at the sampled location\n",
    "    res= two_dim_function_complex(params['burnin_period'],params['learning_rate'])\n",
    "    data =pd.read_csv(path_to_predictions, delimiter =',')\n",
    "    \n",
    "    mean_loss =np.mean(data.loc[0:(len(data)-2),'loss'].values)\n",
    "    std_loss =np.std(data.loc[0:(len(data)-2),'loss'].values)\n",
    "\n",
    "\n",
    "    data.at[len(data)-1, 'loss'] = res    \n",
    "    data.to_csv(path_to_predictions, index=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Plot the data\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(1,2,1, projection='3d')\n",
    "    # Make data.\n",
    "    burnin_period =  np.linspace(*search_space['burnin_period'][0])\n",
    "    learning_rate =  np.linspace(*search_space['learning_rate'][0]) \n",
    "    X, Y = np.meshgrid(burnin_period, learning_rate,indexing='ij')\n",
    "\n",
    "    Z = two_dim_function_complex(X, Y)\n",
    "\n",
    "    # Customize the z axis.\n",
    "    ax.set_zlim(-.5, 1.5)\n",
    "    ax.set_ylim(search_space['learning_rate'][0][0], search_space['learning_rate'][0][1])\n",
    "    ax.set_xlim(search_space['burnin_period'][0][0], search_space['burnin_period'][0][1])\n",
    "    ax.set_xlabel('Burn-in period')\n",
    "    ax.set_ylabel('Learning rate')\n",
    "    ax.set_zlabel('Loss')\n",
    "\n",
    "    ax.scatter(data['burnin_period'].values, data['learning_rate'].values, data['loss'].values, color='b', linewidth=0.5);\n",
    "    ax.scatter(data['burnin_period'].values[-1], data['learning_rate'].values[-1], data['loss'].values[-1], color='r', linewidth=0.5);\n",
    "\n",
    "        \n",
    "    if ii>0:\n",
    "        if ii>1:\n",
    "            loss_predicted= loss_predicted*(std_loss+1e-6)+mean_loss        \n",
    "        \n",
    "        surf_2 = ax.plot_surface(X,Y, loss_predicted, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "        \n",
    "    \n",
    "    ax = fig.add_subplot(1,2,2)\n",
    "    if ii>0:\n",
    "        #fig = plt.figure()\n",
    "        \n",
    "        ax.contour(X,Y,loss_predicted)\n",
    "        ax.contourf(X,Y,loss_predicted, cmap='viridis')\n",
    "        ax.plot(data['burnin_period'].values, data['learning_rate'].values, 'ko')\n",
    "        \n",
    "        ax.plot(data['burnin_period'].values[-1], data['learning_rate'].values[-1], 'ro')\n",
    "        ax.set_xlabel('Burn-in period')\n",
    "        ax.set_ylabel('Learning rate')\n",
    "    else:\n",
    "        ax.contourf(X,Y,np.zeros((np.shape(X))), cmap='viridis')\n",
    "        \n",
    "    ax.set_aspect(1.0/ax.get_data_ratio(), adjustable='box')\n",
    "    fig.tight_layout(pad=6.0)\n",
    "    fig.savefig('figures_2D/'+str(ii)+'_2D_contour.png', bbox_inches='tight',dpi=300)\n",
    "\n",
    "\n",
    "images = []\n",
    "for ii in range(1,number_of_iterations):\n",
    "    filename = 'figures_2D/'+str(ii)+'_2D_contour.png'\n",
    "    image = imageio.imread(filename)\n",
    "    images.append(image)\n",
    "        \n",
    "imageio.mimsave(('fit_2D.gif'), images, duration = .5) # modify duration as needed\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
